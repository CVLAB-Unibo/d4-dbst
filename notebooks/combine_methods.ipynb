{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of merge_methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.10 64-bit ('.venv': poetry)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "703f0cf90f7c0b32ef7a516f9b9135201f6354f648354ddd94775517d718f58f"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import os\n",
        "import sys\n",
        "# sys.path.append(\".\")\n",
        "os.chdir('..')\n",
        "import numpy as np\n",
        "import torch\n",
        "# import torch.optim as optim\n",
        "from model.net import get_network, get_transfer, get_adaptive_network\n",
        "from model.deeplab import Res_Deeplab\n",
        "# from model.bdl import Deeplab\n",
        "# from model.mrnet import DeeplabMulti as mrnet\n",
        "# from model.max_squares import DeeplabMulti as maxsq\n",
        "from model.proda_backbone import Deeplab as DeepLab_proto\n",
        "# from model.classifier import ASPP_Classifier_V2\n",
        "# from model.feature_extractor import resnet_feature_extractor\n",
        "import torch.nn.functional as F\n",
        "from utils.metrics import ScoreUpdater, Accuracy\n",
        "import utils.utils as utils\n",
        "from dataloader.cityscapes_test import CS_test\n",
        "from torch.utils import data\n",
        "from tqdm import tqdm\n",
        "# import cv2\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "# from PIL import Image as pil\n",
        "# import matplotlib.pyplot as plt\n",
        "# from IPython.display import Image, display, clear_output\n",
        "# from collections import OrderedDict\n",
        "from model.sync_batchnorm import SynchronizedBatchNorm2d\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "# torch.backends.cudnn.deterministic = True"
      ],
      "outputs": [],
      "metadata": {
        "id": "jSXoF7atdLQU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "CityscapesClass = namedtuple('CityscapesClass', ['name', 'id', 'train_id', 'category', 'category_id',\n",
        "                                                    'has_instances', 'ignore_in_eval', 'color'])\n",
        "encoding = [\n",
        "        CityscapesClass('unlabeled',            0, 19, 'void', 0, False, True, (0, 0, 0)),\n",
        "        CityscapesClass('ego vehicle',          1, 19, 'void', 0, False, True, (0, 0, 0)),\n",
        "        CityscapesClass('rectification border', 2, 19, 'void', 0, False, True, (0, 0, 0)),\n",
        "        CityscapesClass('out of roi',           3, 19, 'void', 0, False, True, (0, 0, 0)),\n",
        "        CityscapesClass('static',               4, 19, 'void', 0, False, True, (0, 0, 0)),\n",
        "        CityscapesClass('dynamic',              5, 19, 'void', 0, False, True, (111, 74, 0)),\n",
        "        CityscapesClass('ground',               6, 19, 'void', 0, False, True, (81, 0, 81)),\n",
        "        CityscapesClass('road',                 7, 0, 'flat', 1, False, False, (128, 64, 128)),\n",
        "        CityscapesClass('sidewalk',             8, 1, 'flat', 1, False, False, (244, 35, 232)),\n",
        "        CityscapesClass('parking',              9, 19, 'flat', 1, False, True, (250, 170, 160)),\n",
        "        CityscapesClass('rail track',           10, 19, 'flat', 1, False, True, (230, 150, 140)),\n",
        "        CityscapesClass('building',             11, 2, 'construction', 2, False, False, (70, 70, 70)),\n",
        "        CityscapesClass('wall',                 12, 3, 'construction', 2, False, False, (102, 102, 156)),\n",
        "        CityscapesClass('fence',                13, 4, 'construction', 2, False, False, (190, 153, 153)),\n",
        "        CityscapesClass('guard rail',           14, 19, 'construction', 2, False, True, (180, 165, 180)),\n",
        "        CityscapesClass('bridge',               15, 19, 'construction', 2, False, True, (150, 100, 100)),\n",
        "        CityscapesClass('tunnel',               16, 19, 'construction', 2, False, True, (150, 120, 90)),\n",
        "        CityscapesClass('pole',                 17, 5, 'object', 3, False, False, (153, 153, 153)),\n",
        "        CityscapesClass('polegroup',            18, 19, 'object', 3, False, True, (153, 153, 153)),\n",
        "        CityscapesClass('traffic light',        19, 6, 'object', 3, False, False, (250, 170, 30)),\n",
        "        CityscapesClass('traffic sign',         20, 7, 'object', 3, False, False, (220, 220, 0)),\n",
        "        CityscapesClass('vegetation',           21, 8, 'nature', 4, False, False, (107, 142, 35)),\n",
        "        CityscapesClass('terrain',              22, 9, 'nature', 4, False, False, (152, 251, 152)),\n",
        "        CityscapesClass('sky',                  23, 10, 'sky', 5, False, False, (70, 130, 180)),\n",
        "        CityscapesClass('person',               24, 11, 'human', 6, True, False, (220, 20, 60)),\n",
        "        CityscapesClass('rider',                25, 12, 'human', 6, True, False, (255, 0, 0)),\n",
        "        CityscapesClass('car',                  26, 13, 'vehicle', 7, True, False, (255, 255, 255)),\n",
        "        CityscapesClass('truck',                27, 14, 'vehicle', 7, True, False, (0, 0, 70)),\n",
        "        CityscapesClass('bus',                  28, 15, 'vehicle', 7, True, False, (0, 60, 100)),\n",
        "        CityscapesClass('caravan',              29, 19, 'vehicle', 7, True, True, (0, 0, 90)),\n",
        "        CityscapesClass('trailer',              30, 19, 'vehicle', 7, True, True, (0, 0, 110)),\n",
        "        CityscapesClass('train',                31, 16, 'vehicle', 7, True, False, (0, 80, 100)),\n",
        "        CityscapesClass('motorcycle',           32, 17, 'vehicle', 7, True, False, (0, 0, 230)),\n",
        "        CityscapesClass('bicycle',              33, 18, 'vehicle', 7, True, False, (119, 11, 32)),\n",
        "        CityscapesClass('unknown',              34, 19, 'void', 7, True, False, (0, 0, 0)),\n",
        "        CityscapesClass('license plate',        -1, 19, 'vehicle', 7, False, True, (0, 0, 0)),\n",
        "    ]\n",
        "\n",
        "palette = []\n",
        "colors = {cs_class.train_id: cs_class.color for cs_class in encoding}\n",
        "for train_id, color in sorted(colors.items(), key=lambda item: item[0]):\n",
        "    R, G, B = color\n",
        "    palette.extend((R, G, B))\n",
        "\n",
        "zero_pad = 256 * 3 - len(palette)\n",
        "for i in range(zero_pad):\n",
        "    palette.append(0)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Otbw_adKfBvz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "device = torch.device('cuda:3')\n",
        "num_classes = 19\n",
        "root = \"/home/acardace/projects/atdt-da\"\n",
        "\n",
        "data_dir = os.path.join(\"/media/data3/atdt\")\n",
        "txt_val = os.path.join(root, \"splits/cityscapes/val.txt\")\n",
        "ckpt_filename = \"checkpoint.tar\"\n",
        "best_ckpt_filename = \"model_best.tar\"\n",
        "model_dir_source = os.path.join(root, \"gta2cs/net1_original_high_res\")\n",
        "model_dir_target = os.path.join(root, \"gta2cs/net2_r50_wc_strong/ckpt/gta_src.pth\")\n",
        "\n",
        "model_dir_transfer = os.path.join(root, \"gta2cs/transfer_net1_original_high_res2net2_r50_wc_strong_long\")\n",
        "model_dir_baseline2 = os.path.join(root, \"gta2cs/DA/proto/from_gta5_to_cityscapes_on_deeplabv2_best_model.pkl\")\n",
        "# model_dir_baseline = os.path.join(root, \"gta2cs/DA/max_square/GTA5_to_Cityscapes_MaxSquare.pth\")\n",
        "# model_dir_baseline = os.path.join(root, \"gta2cs/DA/adaptsegnet/GTA5_multi.pth\")\n",
        "# model_dir_baseline = os.path.join(root, \"gta2cs/DA/bdl/gta5_ssl.pth\")\n",
        "# model_dir_baseline2 = os.path.join(root, \"gta2cs/DA/ltir/ResNet_GTA_50.2.pth\")\n",
        "# model_dir_baseline2 = os.path.join(root, \"gta2cs/DA/stuff_and_things/BestGTA5_post_SSL.pth\")\n",
        "# model_dir_baseline2 = os.path.join(root, \"gta2cs/DA/bdl/gta_2_city_deeplab.pth\")\n",
        "# model_dir_baseline = os.path.join(root, \"gta2cs/DA/fada/g2c_sd.pth\")\n",
        "# model_dir_baseline0 = \"/home/acardace/projects/gta2cs/target_only_TA_augmented_mrnet/ckpt/checkpoint.tar\"\n",
        "\n",
        "json_path = os.path.join(model_dir_transfer, 'params.json')\n",
        "params = utils.Params(json_path)\n",
        "params.device = device\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "cZUe5W2YgFJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# val_dl = dataloader.fetch_dataloader(data_dir, txt_val, 'val', params)\n",
        "mean=np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
        "std=np.array((1, 1, 1), dtype=np.float32)\n",
        "\n",
        "dataset = CS_test(root=data_dir, txt_file=txt_val, size=(2048, 1024), label_size=(2048, 1024), mean=np.array((0, 0, 0), dtype=np.float32), std=np.array((1, 1, 1), dtype=np.float32), rgb=True, interpolation=\"bilinear\")\n",
        "val_dl = data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "dataset1 = CS_test(root=data_dir, txt_file=txt_val, size=(1024, 512), label_size=(2048, 1024), interpolation=\"lanczos\")\n",
        "val_dl1 = data.DataLoader(dataset1, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "model_source = Res_Deeplab(num_classes=1, use_sigmoid=True).to(device)\n",
        "model_target = Res_Deeplab(num_classes=19).to(device)\n",
        "# model_baseline2 = mrnet(num_classes=19, use_se=True, train_bn=False, norm_style=\"gn\", droprate=0.2).to(device)\n",
        "# model_baseline2 = maxsq(num_classes=19).to(device)\n",
        "# model_baseline = maxsq(num_classes=19).to(device)\n",
        "# model_baseline = Deeplab(num_classes=19).to(device)\n",
        "# model_baseline2 = Deeplab(num_classes=19).to(device)\n",
        "\n",
        "# model_baseline2 = Deeplab(num_classes=19).to(device)\n",
        "# model_baseline2 = Deeplab(num_classes=19).to(device)\n",
        "# model_baseline0 = Res_Deeplab(num_classes=19, layers=23).to(device)\n",
        "\n",
        "transfer = get_transfer().to(device)"
      ],
      "outputs": [],
      "metadata": {
        "id": "1NRiJjkgg66J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "ckpt_source_file_path = os.path.join(model_dir_source, 'ckpt', best_ckpt_filename)\n",
        "saved_state_dict = torch.load(ckpt_source_file_path, map_location=device)[\"state_dict\"]\n",
        "model_source.load_state_dict(saved_state_dict)\n",
        "# model_source = utils.load_checkpoint(model_source, ckpt_dir=ckpt_source_file_path, filename=best_ckpt_filename, is_best=True)[0]\n",
        "\n",
        "saved_state_dict = torch.load(os.path.join(model_dir_target), map_location=device)\n",
        "model_target.load_state_dict(saved_state_dict)\n",
        "\n",
        "ckpt_transfer_file_path = os.path.join(model_dir_transfer, 'ckpt', best_ckpt_filename)\n",
        "saved_state_dict = torch.load(ckpt_transfer_file_path, map_location=device)[\"state_dict\"]\n",
        "transfer.load_state_dict(saved_state_dict)\n",
        "\n",
        "#adaptive model \n",
        "source_encoder = torch.nn.Sequential(*(list(model_source.children())[:-2]))\n",
        "target_encoder = torch.nn.Sequential(*(list(model_target.children())[:-1]))\n",
        "target_decoder = list(model_target.children())[-1]\n",
        "adaptive_model = get_adaptive_network(source_encoder, transfer, target_decoder)\n",
        "\n",
        "# ckpt = torch.load(\"/content/drive/My Drive/projects/atdt/gta2cs/fn_transfer_decoder_transfer_43.1/ckpt/checkpoint.tar\")\n",
        "# adaptive_model.load_state_dict(ckpt[\"state_dict\"])\n",
        "# saved_state_dict = torch.load(model_dir_baseline0, map_location=device)\n",
        "# model_baseline0.load_state_dict(saved_state_dict[\"state_dict\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "EhF686oPRepg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# proto -> DeepLab_proto, bicubic, bicubic\n",
        "BatchNorm = SynchronizedBatchNorm2d\n",
        "model_baseline2 = DeepLab_proto(BatchNorm, num_classes=19, freeze_bn=False, restore_from=model_dir_baseline2, bn_clr=True).to(device)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# #maxsquare -> maxsq, bicubic,  _\n",
        "# saved_state_dict = torch.load(model_dir_baseline2, map_location=device)[\"state_dict\"]\n",
        "# new_params = {'.'.join(k.split('.')[1:]) : v for k, v in saved_state_dict.items()}\n",
        "# model_baseline2.load_state_dict(new_params)"
      ],
      "outputs": [],
      "metadata": {
        "id": "HvFL_PFuRfwF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# staff and things -> maxsq, bicubic, bicubic\n",
        "# saved_state_dict = torch.load(model_dir_baseline2, map_location=device)\n",
        "# model_baseline2.load_state_dict(saved_state_dict)"
      ],
      "outputs": [],
      "metadata": {
        "id": "oIVfnQykRg_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# ltir -> Deeplab, _ ,bicubic\n",
        "# saved_state_dict = torch.load(model_dir_baseline2, map_location=device)\n",
        "# model_baseline2.load_state_dict(saved_state_dict)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MJI2uPCB8QXj",
        "outputId": "12326f61-1fea-4e12-bb07-4d433ad49924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "#bdl -> Deeplab, lanczos, lanczos\n",
        "# saved_state_dict = torch.load(model_dir_baseline2, map_location=device)\n",
        "# model_baseline2.load_state_dict(saved_state_dict)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KQoIIGOvRhEH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "#adaptsegnet -> maxsq, bicubic, _\n",
        "# saved_state_dict = torch.load(model_dir_baseline, map_location=device)\n",
        "# model_baseline.load_state_dict(saved_state_dict)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KJCQaxwtRhHp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# # #mrnet -> mrnet, lanczos  ,lanczos\n",
        "# saved_state_dict = torch.load(model_dir_baseline2, map_location=device)\n",
        "# new_params = {'.'.join(k.split('.')[1:]) : v for k, v in saved_state_dict.items()}\n",
        "# model_baseline2.load_state_dict(new_params)"
      ],
      "outputs": [],
      "metadata": {
        "id": "1rTn9_LcRhBz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# # fada, bicubic, lanczos\n",
        "# def strip_prefix_if_present(state_dict, prefix):\n",
        "#     keys = sorted(state_dict.keys())\n",
        "#     if not all(key.startswith(prefix) for key in keys):\n",
        "#         return state_dict\n",
        "#     stripped_state_dict = OrderedDict()\n",
        "#     for key, value in state_dict.items():\n",
        "#         stripped_state_dict[key.replace(prefix, \"\")] = value\n",
        "#     return stripped_state_dict\n",
        "\n",
        "# def build_feature_extractor():\n",
        "#     backbone = resnet_feature_extractor(\"resnet101\", pretrained_weights=\"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\", aux=False, pretrained_backbone=True, freeze_bn=False)\n",
        "#     return backbone\n",
        "\n",
        "# def build_classifier():\n",
        "#     classifier = ASPP_Classifier_V2(2048, [6, 12, 18, 24], [6, 12, 18, 24], 19)\n",
        "#     return classifier\n",
        "\n",
        "# feature_extractor = build_feature_extractor()\n",
        "# feature_extractor.to(device)\n",
        "\n",
        "# classifier = build_classifier()\n",
        "# classifier.to(device)\n",
        "# checkpoint = torch.load(model_dir_baseline, map_location=torch.device('cuda'))\n",
        "# feature_extractor_weights = strip_prefix_if_present(checkpoint['feature_extractor'], 'module.')\n",
        "# feature_extractor.load_state_dict(feature_extractor_weights)\n",
        "# classifier_weights = strip_prefix_if_present(checkpoint['classifier'], 'module.')\n",
        "# classifier.load_state_dict(classifier_weights)\n",
        "\n",
        "# model_baseline = torch.nn.Sequential(feature_extractor, classifier)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Hjwrp__VXcPJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "def generate(adpative_model, model_baseline, val_dl, val_dl1, params):\n",
        "    # set model to evaluation mode\n",
        "        adpative_model.eval()\n",
        "        model_baseline.eval()\n",
        "        valid_labels = range(19)\n",
        "        x_num = 500\n",
        "\n",
        "        scorer = ScoreUpdater(valid_labels, params.num_classes, x_num, None)\n",
        "        acc = Accuracy(params.num_classes+1, ignore_index=19)\n",
        "        scorer.reset()\n",
        "        acc.reset()\n",
        "        classes = np.arange(num_classes)\n",
        "        inverted_w = np.load('weights/inverted_weights_gta1.npy')\n",
        "        baseline_weights = inverted_w/inverted_w.max()\n",
        "        model_weights = 1-(inverted_w/inverted_w.max())\n",
        "        iter_transfer = iter(val_dl1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (xb, yb, _, name) in enumerate(tqdm(val_dl)):\n",
        "\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                # print(xb.size())\n",
        "                xb_transfer, _, _, _ =  next(iter_transfer)\n",
        "                xb_transfer = xb_transfer.to(device)\n",
        "      \n",
        "                z = adpative_model(xb_transfer)\n",
        "                z = F.interpolate(z, size=(params.label_size[1],params.label_size[0]), mode='bilinear', align_corners=True)\n",
        "                z = F.softmax(z/6, dim=1)\n",
        "                \n",
        "                # rescale transfer with softmax based on TA weights\n",
        "                prediction_z = torch.argmax(z, dim=1)\n",
        "                mask_z = torch.zeros(prediction_z.size()).to(params.device)\n",
        "                for label in classes:\n",
        "                    mask_z[prediction_z.eq(label)] = model_weights[label]\n",
        "\n",
        "                z *= mask_z.unsqueeze(dim=1)\n",
        "    \n",
        "                model_target_out = model_baseline(xb)\n",
        "                model_target_out = F.interpolate(model_target_out, size=(params.label_size[1],params.label_size[0]), mode='bilinear', align_corners=True)\n",
        "                model_target_out = F.softmax(model_target_out/6, dim=1)\n",
        "\n",
        "                prediction_baseline = torch.argmax(model_target_out, dim=1)\n",
        "                mask_baseline = torch.zeros(prediction_baseline.size()).to(params.device)\n",
        "                for label in classes:\n",
        "                    mask_baseline[prediction_baseline.eq(label)] = baseline_weights[label]\n",
        "                model_target_out *= mask_baseline.unsqueeze(dim=1)\n",
        "\n",
        "                out = z + model_target_out \n",
        "                out = F.softmax(out, dim=1)\n",
        "\n",
        "                output = out[0].cpu().numpy()\n",
        "                output = output.transpose(1, 2, 0)\n",
        "                label, prob = np.argmax(output, axis=2), np.max(output, axis=2)\n",
        "\n",
        "                scorer.update(label.flatten(), yb[0].cpu().numpy().flatten(), i)\n",
        "                acc.add(out, yb)\n",
        "        scorer.print_score()\n",
        "        acc = acc.value()\n",
        "        print('acc:', acc)\n",
        "        "
      ],
      "outputs": [],
      "metadata": {
        "id": "S3W6FerqiE9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "generate(adaptive_model, model_baseline2, val_dl, val_dl1, params)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 13/500 [00:14<07:27,  1.09it/s]"
          ]
        }
      ],
      "metadata": {
        "id": "8VsAoVkLjNk8",
        "outputId": "0a6a7d60-a736-4d2e-8bdf-12e168228f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "2BKtiNEJ6la8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ]
}