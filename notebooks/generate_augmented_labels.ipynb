{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.3 64-bit",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "916cb98624b1d6b98c2e30b87afe803b904cdac716ce275089a85f0040abf4a6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from model.net import get_network, get_transfer, get_adaptive_network\n",
    "from model.deeplab import Res_Deeplab\n",
    "from model.bdl import Deeplab\n",
    "from model.mrnet import DeeplabMulti as mrnet\n",
    "from model.max_squares import DeeplabMulti as maxsq\n",
    "from model.classifier import ASPP_Classifier_V2\n",
    "from model.feature_extractor import resnet_feature_extractor\n",
    "import torch.nn.functional as F\n",
    "from utils.metrics import ScoreUpdater, Accuracy\n",
    "import utils.utils as utils\n",
    "import dataloader.dataloader2 as dataloader\n",
    "from dataset.cityscapes_test import CS_test\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from PIL import Image as pil\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CityscapesClass = namedtuple('CityscapesClass', ['name', 'id', 'train_id', 'category', 'category_id',\n",
    "                                                    'has_instances', 'ignore_in_eval', 'color'])\n",
    "encoding = [\n",
    "        CityscapesClass('unlabeled',            0, 19, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('ego vehicle',          1, 19, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('rectification border', 2, 19, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('out of roi',           3, 19, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('static',               4, 19, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('dynamic',              5, 19, 'void', 0, False, True, (111, 74, 0)),\n",
    "        CityscapesClass('ground',               6, 19, 'void', 0, False, True, (81, 0, 81)),\n",
    "        CityscapesClass('road',                 7, 0, 'flat', 1, False, False, (128, 64, 128)),\n",
    "        CityscapesClass('sidewalk',             8, 1, 'flat', 1, False, False, (244, 35, 232)),\n",
    "        CityscapesClass('parking',              9, 19, 'flat', 1, False, True, (250, 170, 160)),\n",
    "        CityscapesClass('rail track',           10, 19, 'flat', 1, False, True, (230, 150, 140)),\n",
    "        CityscapesClass('building',             11, 2, 'construction', 2, False, False, (70, 70, 70)),\n",
    "        CityscapesClass('wall',                 12, 3, 'construction', 2, False, False, (102, 102, 156)),\n",
    "        CityscapesClass('fence',                13, 4, 'construction', 2, False, False, (190, 153, 153)),\n",
    "        CityscapesClass('guard rail',           14, 19, 'construction', 2, False, True, (180, 165, 180)),\n",
    "        CityscapesClass('bridge',               15, 19, 'construction', 2, False, True, (150, 100, 100)),\n",
    "        CityscapesClass('tunnel',               16, 19, 'construction', 2, False, True, (150, 120, 90)),\n",
    "        CityscapesClass('pole',                 17, 5, 'object', 3, False, False, (153, 153, 153)),\n",
    "        CityscapesClass('polegroup',            18, 19, 'object', 3, False, True, (153, 153, 153)),\n",
    "        CityscapesClass('traffic light',        19, 6, 'object', 3, False, False, (250, 170, 30)),\n",
    "        CityscapesClass('traffic sign',         20, 7, 'object', 3, False, False, (220, 220, 0)),\n",
    "        CityscapesClass('vegetation',           21, 8, 'nature', 4, False, False, (107, 142, 35)),\n",
    "        CityscapesClass('terrain',              22, 9, 'nature', 4, False, False, (152, 251, 152)),\n",
    "        CityscapesClass('sky',                  23, 10, 'sky', 5, False, False, (70, 130, 180)),\n",
    "        CityscapesClass('person',               24, 11, 'human', 6, True, False, (220, 20, 60)),\n",
    "        CityscapesClass('rider',                25, 12, 'human', 6, True, False, (255, 0, 0)),\n",
    "        CityscapesClass('car',                  26, 13, 'vehicle', 7, True, False, (255, 255, 255)),\n",
    "        CityscapesClass('truck',                27, 14, 'vehicle', 7, True, False, (0, 0, 70)),\n",
    "        CityscapesClass('bus',                  28, 15, 'vehicle', 7, True, False, (0, 60, 100)),\n",
    "        CityscapesClass('caravan',              29, 19, 'vehicle', 7, True, True, (0, 0, 90)),\n",
    "        CityscapesClass('trailer',              30, 19, 'vehicle', 7, True, True, (0, 0, 110)),\n",
    "        CityscapesClass('train',                31, 16, 'vehicle', 7, True, False, (0, 80, 100)),\n",
    "        CityscapesClass('motorcycle',           32, 17, 'vehicle', 7, True, False, (0, 0, 230)),\n",
    "        CityscapesClass('bicycle',              33, 18, 'vehicle', 7, True, False, (119, 11, 32)),\n",
    "        CityscapesClass('unknown',              34, 19, 'void', 7, True, False, (0, 0, 0)),\n",
    "        CityscapesClass('license plate',        -1, 19, 'vehicle', 7, False, True, (0, 0, 0)),\n",
    "    ]\n",
    "\n",
    "palette = []\n",
    "colors = {cs_class.train_id: cs_class.color for cs_class in encoding}\n",
    "for train_id, color in sorted(colors.items(), key=lambda item: item[0]):\n",
    "    R, G, B = color\n",
    "    palette.extend((R, G, B))\n",
    "\n",
    "zero_pad = 256 * 3 - len(palette)\n",
    "for i in range(zero_pad):\n",
    "    palette.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "num_classes = 19\n",
    "root = \"/media/data_4t/aCardace\"\n",
    "\n",
    "data_dir = os.path.join(root, \"datasets/\")\n",
    "txt_val = os.path.join(root, \"atdt/splits/cityscapes/val.txt\")\n",
    "ckpt_filename = \"checkpoint.tar\"\n",
    "best_ckpt_filename = \"model_best.tar\"\n",
    "model_dir_source = os.path.join(root, \"atdt/gta2cs/net1_original_high_res\")\n",
    "model_dir_target = os.path.join(root, \"atdt/gta2cs/net2_r50_wc_strong/ckpt/gta_src.pth\")\n",
    "\n",
    "model_dir_transfer = os.path.join(root, \"atdt/gta2cs/transfer_net1_original_high_res2net2_r50_wc_strong_long\")\n",
    "# model_dir_baseline = os.path.join(root, \"atdt/gta2cs/DA/mrnet/stage2.pth\")\n",
    "# model_dir_baseline = os.path.join(root, \"atdt/gta2cs/DA/max_square/GTA5_to_Cityscapes_MaxSquare.pth\")\n",
    "# model_dir_baseline = os.path.join(root, \"atdt/gta2cs/DA/adaptsegnet/GTA5_multi.pth\")\n",
    "# model_dir_baseline = os.path.join(root, \"atdt/gta2cs/DA/bdl/gta5_ssl.pth\")\n",
    "model_dir_baseline = os.path.join(root, \"atdt/gta2cs/DA/ltir/ResNet_GTA_50.2.pth\")\n",
    "# model_dir_baseline = os.path.join(root, \"atdt/gta2cs/DA/stuff_and_things/BestGTA5_post_SSL.pth\")\n",
    "# model_dir_baseline = os.path.join(root, \"atdt/gta2cs/DA/bdl/gta_2_city_deeplab.pth\")\n",
    "# model_dir_baseline = os.path.join(root, \"atdt/gta2cs/DA/fada/g2c_sd.pth\")\n",
    "# model_dir_baseline0 = \"/media/data_4t/aCardace/atdt/gta2cs/target_only_TA_augmented_bdl/ckpt/checkpoint.tar\"\n",
    "\n",
    "json_path = os.path.join(model_dir_transfer, 'params.json')\n",
    "params = utils.Params(json_path)\n",
    "params.device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# val_dl = dataloader.fetch_dataloader(data_dir, txt_val, 'val', params)\n",
    "mean=np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
    "std=np.array((1, 1, 1), dtype=np.float32)\n",
    "\n",
    "dataset = CS_test(root=data_dir, txt_file=\"/media/data_4t/aCardace/atdt/splits/cityscapes/train_random.txt\", use_depth=True, threshold=50, size=(1024, 512), label_size=(1024, 512), mean=mean, std=std, rgb=False, interpolation=\"lanczos\")\n",
    "val_dl = data.DataLoader(dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "dataset1 = CS_test(root=data_dir, txt_file=\"/media/data_4t/aCardace/atdt/splits/cityscapes/train_random.txt\", use_depth=True, threshold=50, size=(1024, 512), label_size=(1024, 512), interpolation=\"lanczos\")\n",
    "val_dl1 = data.DataLoader(dataset1, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "model_source = Res_Deeplab(num_classes=1, use_sigmoid=True).to(device)\n",
    "model_target = Res_Deeplab(num_classes=19).to(device)\n",
    "# model_baseline = mrnet(num_classes=19, use_se=True, train_bn=False, norm_style=\"gn\", droprate=0.2).to(device)\n",
    "# model_baseline = maxsq(num_classes=19).to(device)\n",
    "# model_baseline = maxsq(num_classes=19).to(device)\n",
    "# model_baseline = Deeplab(num_classes=19).to(device)\n",
    "# model_baseline2 = Deeplab(num_classes=19).to(device)\n",
    "\n",
    "model_baseline = Deeplab(num_classes=19).to(device)\n",
    "# model_baseline2 = Deeplab(num_classes=19).to(device)\n",
    "# model_baseline0 = Res_Deeplab(num_classes=19, layers=23).to(device)\n",
    "\n",
    "transfer = get_transfer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_source_file_path = os.path.join(model_dir_source, 'ckpt', best_ckpt_filename)\n",
    "saved_state_dict = torch.load(ckpt_source_file_path, map_location=device)[\"state_dict\"]\n",
    "model_source.load_state_dict(saved_state_dict)\n",
    "# model_source = utils.load_checkpoint(model_source, ckpt_dir=ckpt_source_file_path, filename=best_ckpt_filename, is_best=True)[0]\n",
    "\n",
    "saved_state_dict = torch.load(os.path.join(model_dir_target), map_location=device)\n",
    "model_target.load_state_dict(saved_state_dict)\n",
    "\n",
    "ckpt_transfer_file_path = os.path.join(model_dir_transfer, 'ckpt', best_ckpt_filename)\n",
    "saved_state_dict = torch.load(ckpt_transfer_file_path, map_location=device)[\"state_dict\"]\n",
    "transfer.load_state_dict(saved_state_dict)\n",
    "\n",
    "#adaptive model \n",
    "source_encoder = torch.nn.Sequential(*(list(model_source.children())[:-2]))\n",
    "target_encoder = torch.nn.Sequential(*(list(model_target.children())[:-1]))\n",
    "target_decoder = list(model_target.children())[-1]\n",
    "adaptive_model = get_adaptive_network(source_encoder, transfer, target_decoder)\n",
    "\n",
    "# ckpt = torch.load(\"/content/drive/My Drive/projects/atdt/gta2cs/fn_transfer_decoder_transfer_43.1/ckpt/checkpoint.tar\")\n",
    "# adaptive_model.load_state_dict(ckpt[\"state_dict\"])\n",
    "# saved_state_dict = torch.load(model_dir_baseline0, map_location=device)\n",
    "# model_baseline0.load_state_dict(saved_state_dict[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #maxsquare -> maxsq, bicubic,  _\n",
    "# saved_state_dict = torch.load(model_dir_baseline2, map_location=device)[\"state_dict\"]\n",
    "# new_params = {'.'.join(k.split('.')[1:]) : v for k, v in saved_state_dict.items()}\n",
    "# model_baseline2.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# staff and things -> maxsq, bicubic, bicubic\n",
    "# saved_state_dict = torch.load(model_dir_baseline, map_location=device)\n",
    "# model_baseline.load_state_dict(saved_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ltir -> Deeplab, _ ,bicubic\n",
    "# saved_state_dict = torch.load(model_dir_baseline, map_location=device)\n",
    "# model_baseline.load_state_dict(saved_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#bdl -> Deeplab, lanczos, lanczos\n",
    "saved_state_dict = torch.load(model_dir_baseline, map_location=device)\n",
    "model_baseline.load_state_dict(saved_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adaptsegnet -> maxsq, bicubic, _\n",
    "# saved_state_dict = torch.load(model_dir_baseline, map_location=device)\n",
    "# model_baseline.load_state_dict(saved_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #mrnet -> mrnet, lanczos  ,lanczos\n",
    "# saved_state_dict = torch.load(model_dir_baseline, map_location=device)\n",
    "# new_params = {'.'.join(k.split('.')[1:]) : v for k, v in saved_state_dict.items()}\n",
    "# model_baseline.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fada, bicubic, lanczos\n",
    "# def strip_prefix_if_present(state_dict, prefix):\n",
    "#     keys = sorted(state_dict.keys())\n",
    "#     if not all(key.startswith(prefix) for key in keys):\n",
    "#         return state_dict\n",
    "#     stripped_state_dict = OrderedDict()\n",
    "#     for key, value in state_dict.items():\n",
    "#         stripped_state_dict[key.replace(prefix, \"\")] = value\n",
    "#     return stripped_state_dict\n",
    "\n",
    "# def build_feature_extractor():\n",
    "#     backbone = resnet_feature_extractor(\"resnet101\", pretrained_weights=\"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\", aux=False, pretrained_backbone=True, freeze_bn=False)\n",
    "#     return backbone\n",
    "\n",
    "# def build_classifier():\n",
    "#     classifier = ASPP_Classifier_V2(2048, [6, 12, 18, 24], [6, 12, 18, 24], 19)\n",
    "#     return classifier\n",
    "\n",
    "# feature_extractor = build_feature_extractor()\n",
    "# feature_extractor.to(device)\n",
    "\n",
    "# classifier = build_classifier()\n",
    "# classifier.to(device)\n",
    "# checkpoint = torch.load(model_dir_baseline, map_location=torch.device('cuda'))\n",
    "# feature_extractor_weights = strip_prefix_if_present(checkpoint['feature_extractor'], 'module.')\n",
    "# feature_extractor.load_state_dict(feature_extractor_weights)\n",
    "# classifier_weights = strip_prefix_if_present(checkpoint['classifier'], 'module.')\n",
    "# classifier.load_state_dict(classifier_weights)\n",
    "\n",
    "# model_baseline = torch.nn.Sequential(feature_extractor, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_mask(pred, classes):\n",
    "    pred, classes = torch.broadcast_tensors(pred.unsqueeze(0), classes.unsqueeze(1).unsqueeze(2))\n",
    "    N = pred.eq(classes).sum(0)\n",
    "    return N\n",
    "\n",
    "def update_mask_with_depth(mask, depth):\n",
    "\n",
    "    depth_clone = depth.clone()\n",
    "    eps = 1e-4\n",
    "    b, h, w = mask.size()\n",
    "    p_t = np.percentile(depth[-1].numpy(), 85)\n",
    "    depth_clone[-1][depth_clone[-1]>p_t] = p_t+eps\n",
    "    depth_clone[-1] = (depth_clone[-1] - depth_clone[-1].min())/(depth_clone[-1].max() - depth_clone[-1].min()) \n",
    "\n",
    "    depth_masks = []\n",
    "    for i in range(depth.shape[0]-1):\n",
    "        p = np.percentile(depth[i].numpy(), 85)\n",
    "        depth_clone[i][depth_clone[i]>p] = p+eps   \n",
    "        depth_clone[i] = (depth_clone[i] - depth_clone[i].min())/(depth_clone[i].max() - depth_clone[i].min()) \n",
    "        close_pixels = torch.le(depth_clone[i], p)\n",
    "        # le = torch.lt(depth_clone[i], depth_clone[-1])\n",
    "        # close_pixels = close_pixels & le \n",
    "        depth_masks.append(close_pixels)\n",
    "    depth_masks = torch.stack(depth_masks, dim=0)\n",
    "    \n",
    "    updated_mask = mask * depth_masks\n",
    "    return updated_mask, depth_clone\n",
    "\n",
    "def cross_check_mask(mask, normalized_depth):\n",
    "    sorted_indexes = torch.argsort(normalized_depth, dim=0, descending=False)\n",
    "    \n",
    "    original_mask = mask.clone()\n",
    "    _, h, w = original_mask.size() \n",
    "    original_mask = torch.cat([original_mask, torch.full((1,h,w), 1, dtype=torch.int)], dim=0)\n",
    "    masks = []\n",
    "\n",
    "    for k in range(original_mask.shape[0]):\n",
    "        masks.append([])\n",
    "\n",
    "    for i in range(sorted_indexes.shape[0]):\n",
    "        for j in range(original_mask.shape[0]):\n",
    "            closest = sorted_indexes[i] == j\n",
    "\n",
    "            mask_j = original_mask[j] & closest\n",
    "            masks[j].append(mask_j)\n",
    "            _, mask_j = torch.broadcast_tensors(original_mask, mask_j)\n",
    "            mask_j = ~mask_j\n",
    "            mask_j[j] = True\n",
    "            mask_j[-1] = True\n",
    "            original_mask = original_mask & mask_j\n",
    "\n",
    "    final_masks = []\n",
    "    for k in range(mask.shape[0]):\n",
    "        mask_k = torch.stack(masks[k], dim=0).numpy()\n",
    "        mask_k = np.any(mask_k, axis=0)\n",
    "        final_masks.append(torch.tensor(mask_k))\n",
    "\n",
    "    final_masks = torch.stack(final_masks, dim=0)\n",
    "    return final_masks.long()\n",
    "\n",
    "def oneMix(mask, source, target):\n",
    "    stackedMask0, _ = torch.broadcast_tensors(mask, source)\n",
    "    return stackedMask0*source+(1-stackedMask0)*target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(adaptive_model, model_baseline, val_dl, val_dl1, params):\n",
    "    # set model to evaluation mode\n",
    "        adaptive_model.eval()\n",
    "        model_baseline.eval()\n",
    "        # plt.figure(figsize=(30,20))\n",
    "\n",
    "        take_classes = torch.tensor([3,4, 5,6,7,11, 12, 16, 15, 14, 17,18])\n",
    "        valid_labels = range(19)\n",
    "        x_num = 500\n",
    "\n",
    "        scorer = ScoreUpdater(valid_labels, params.num_classes, x_num, None)\n",
    "        acc = Accuracy(params.num_classes+1, ignore_index=19)\n",
    "        scorer.reset()\n",
    "        acc.reset()     \n",
    "\n",
    "        classes = np.arange(num_classes)\n",
    "        inverted_w = np.load('/media/data_4t/aCardace/datasets/gta1/inverted_weights.npy')\n",
    "        baseline_weights = inverted_w/inverted_w.max()\n",
    "        model_weights = 1-(inverted_w/inverted_w.max())\n",
    "        iter_transfer = iter(val_dl1)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (xb, yb, depth, name) in enumerate(tqdm(val_dl, position=0, leave=True)):\n",
    "                stack_masks = []\n",
    "\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                xb_transfer, _, _ , _ = next(iter_transfer)\n",
    "                xb_transfer = xb_transfer.to(device)\n",
    "\n",
    "                outs = []\n",
    "                for h in range(0, xb.shape[0], 2):\n",
    "                    \n",
    "                    start_i = h\n",
    "                    end_i = start_i+2\n",
    "\n",
    "                    z = adaptive_model(xb_transfer[start_i:end_i])\n",
    "                    z = F.interpolate(z, size=(params.load_size[1],params.load_size[0]), mode='bilinear', align_corners=True)\n",
    "                    z = F.softmax(z/6, dim=1)\n",
    "                    \n",
    "                    prediction_z = torch.argmax(z, dim=1)\n",
    "                    mask_z = torch.zeros(prediction_z.size()).to(params.device)\n",
    "                    for label in classes:\n",
    "                        mask_z[prediction_z.eq(label)] = model_weights[label]\n",
    "\n",
    "                    z *= mask_z.unsqueeze(dim=1)\n",
    "        \n",
    "                    model_target_out = model_baseline(xb[start_i:end_i])\n",
    "                    model_target_out = F.interpolate(model_target_out, size=(params.load_size[1],params.load_size[0]), mode='bilinear', align_corners=True)\n",
    "                    model_target_out = F.softmax(model_target_out/6, dim=1)\n",
    "\n",
    "                    prediction_baseline = torch.argmax(model_target_out, dim=1)\n",
    "                    mask_baseline = torch.zeros(prediction_baseline.size()).to(params.device)\n",
    "                    for label in classes:\n",
    "                        mask_baseline[prediction_baseline.eq(label)] = baseline_weights[label]\n",
    "                    model_target_out *= mask_baseline.unsqueeze(dim=1)\n",
    "\n",
    "                    out = model_target_out \n",
    "                    out = F.softmax(out, dim=1)\n",
    "                    outs.append(out)\n",
    "\n",
    "                outs = torch.cat(outs, dim=0)\n",
    "                # print(\"outs\", outs.size())\n",
    "                # output = out[0].cpu().numpy()\n",
    "                # output = output.transpose(1, 2, 0)\n",
    "                # label, prob = np.argmax(output, axis=2), np.max(output, axis=2)\n",
    "\n",
    "                # scorer.update(label.flatten(), yb[0].cpu().numpy().flatten(), i)\n",
    "\n",
    "                out = outs.cpu()\n",
    "                xb = xb.cpu()\n",
    "                yb = yb.cpu()\n",
    "                depth = depth.cpu()\n",
    "                prediction = torch.argmax(out, dim=1).cpu()\n",
    "\n",
    "                for k in range(prediction.shape[0]-1):\n",
    "                    stack_masks.append(generate_class_mask(prediction[k].cpu(), take_classes))\n",
    "                mask = torch.stack(stack_masks, dim=0)\n",
    "                \n",
    "                mask, normalized_depth = update_mask_with_depth(mask, depth)\n",
    "                # mask = cross_check_mask(mask, normalized_depth)\n",
    "                \n",
    "                a_xb = xb[-1].cpu()\n",
    "                a_prediction = prediction[-1].cpu()\n",
    "                a_yb = yb[-1].cpu()\n",
    "                a_depth = depth[-1].cpu()\n",
    "\n",
    "                for j in range(prediction.shape[0]-1):        \n",
    "                    a_xb = oneMix(mask[j], xb[j].cpu(), a_xb)\n",
    "                    a_prediction = oneMix(mask[j], prediction[j].cpu(), a_prediction)\n",
    "                    a_yb = oneMix(mask[j], yb[j].cpu(), a_yb)\n",
    "                    a_depth = oneMix(mask[j], depth[j].cpu(), a_depth)\n",
    "\n",
    "                #### VISUALIZATION\n",
    "                xb = torch.cat([xb, a_xb.unsqueeze(0)], dim=0)\n",
    "                yb = torch.cat([yb, a_yb.unsqueeze(0)], dim=0)\n",
    "                depth = torch.cat([depth, a_depth.unsqueeze(0)], dim=0)\n",
    "                prediction = torch.cat([prediction, a_prediction.unsqueeze(0)], dim=0)\n",
    "\n",
    "                figure = val_dl.dataset.get_predictions_plot_sem(xb, prediction, yb, depth)\n",
    "                image = pil.fromarray(figure)\n",
    "                image.save(f\"/media/data_4t/aCardace/atdt/test/{i}.png\")\n",
    "\n",
    "                if i>20:\n",
    "                    break\n",
    "\n",
    "                #### VISUALIZATION\n",
    "\n",
    "                # output = np.asarray(a_prediction.cpu().numpy(), dtype=np.uint8)\n",
    "                # output = pil.fromarray(output)\n",
    "                # name = name[0].replace('gtFine', 'gtFine_augmented_msources_nodepth', 1)\n",
    "                # image_name = \"/\".join(name.split('/')[-3:])\n",
    "                # name = name.replace(image_name, f'{i}.png')\n",
    "                # output.save(name)\n",
    "                \n",
    "                # img = np.asarray(a_xb.cpu())\n",
    "                # img = val_dl.dataset.re_normalize(img)\n",
    "                # img = pil.fromarray(img)\n",
    "                # name = name.replace('gtFine_augmented_msources_nodepth', \"train_augmented_msources_nodepth\")\n",
    "                # os.makedirs(os.path.dirname(name), exist_ok=True)\n",
    "                # img.save(name)\n",
    "                # break\n",
    "\n",
    "        # scorer.print_score()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 1/3719 [00:25<26:08:03, 25.30s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2a9cbc05a64a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madaptive_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-3d49527ee65f>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(adaptive_model, model_baseline, val_dl, val_dl1, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mend_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madaptive_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb_transfer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data_4t/aCardace/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data_4t/aCardace/atdt/model/net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data_4t/aCardace/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data_4t/aCardace/torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data_4t/aCardace/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data_4t/aCardace/torch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data_4t/aCardace/torch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "generate(adaptive_model, model_baseline, val_dl, val_dl1, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}